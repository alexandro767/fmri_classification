{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {},
        "id": "ugTOHeL7_lyc"
      },
      "outputs": [],
      "source": [
        "# @title Install dependencies and basic parameters\n",
        "!pip install nilearn -qqq --progress-bar off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {},
        "id": "P74X3TfA_lyd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tarfile\n",
        "import nilearn\n",
        "import nilearn.decoding\n",
        "\n",
        "# Necessary for visualization\n",
        "from nilearn import plotting, datasets\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {},
        "id": "ekEiONEg_lye"
      },
      "outputs": [],
      "source": [
        "# The download cells will store the data in nested directories starting here:\n",
        "HCP_DIR = \"./DATA\"\n",
        "if not os.path.isdir(HCP_DIR):\n",
        "  os.mkdir(HCP_DIR)\n",
        "\n",
        "# The data shared for NMA projects is a subset of the full HCP dataset\n",
        "N_SUBJECTS = 339\n",
        "# N_SUBJECTS = 1200\n",
        "\n",
        "# The data have already been aggregated into ROIs from the Glasesr parcellation\n",
        "N_PARCELS = 360\n",
        "\n",
        "# The acquisition parameters for all tasks were identical\n",
        "TR = 0.72  # Time resolution, in sec\n",
        "\n",
        "# The parcels are matched across hemispheres with the same order\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "\n",
        "# Each experiment was repeated multiple times in each subject\n",
        "N_RUNS_REST = 4\n",
        "N_RUNS_TASK = 2\n",
        "\n",
        "# Time series data are organized by experiment, with each experiment\n",
        "# having an LR and RL (phase-encode direction) acquistion\n",
        "BOLD_NAMES = [\n",
        "  \"rfMRI_REST1_LR\", \"rfMRI_REST1_RL\",\n",
        "  \"rfMRI_REST2_LR\", \"rfMRI_REST2_RL\",\n",
        "  \"tfMRI_MOTOR_RL\", \"tfMRI_MOTOR_LR\",\n",
        "  \"tfMRI_WM_RL\", \"tfMRI_WM_LR\",\n",
        "  \"tfMRI_EMOTION_RL\", \"tfMRI_EMOTION_LR\",\n",
        "  \"tfMRI_GAMBLING_RL\", \"tfMRI_GAMBLING_LR\",\n",
        "  \"tfMRI_LANGUAGE_RL\", \"tfMRI_LANGUAGE_LR\",\n",
        "  \"tfMRI_RELATIONAL_RL\", \"tfMRI_RELATIONAL_LR\",\n",
        "  \"tfMRI_SOCIAL_RL\", \"tfMRI_SOCIAL_LR\"\n",
        "]\n",
        "\n",
        "# You may want to limit the subjects used during code development.\n",
        "# This will use all subjects:\n",
        "subjects = range(N_SUBJECTS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {},
        "id": "xwj0KgDy_lye",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "992191f3-a0d2-4a8d-903c-6ee72dbf8a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hcp_rest.tgz...\n",
            "Download hcp_rest.tgz completed!\n",
            "Downloading hcp_task.tgz...\n",
            "Download hcp_task.tgz completed!\n",
            "Downloading hcp_covariates.tgz...\n",
            "Download hcp_covariates.tgz completed!\n",
            "Downloading atlas.npz...\n",
            "Download atlas.npz completed!\n",
            "Extracting hcp_covariates.tgz...\n",
            "Extracting hcp_rest.tgz...\n",
            "Extracting hcp_task.tgz...\n"
          ]
        }
      ],
      "source": [
        "fnames = [\"hcp_rest.tgz\",\n",
        "          \"hcp_task.tgz\",\n",
        "          \"hcp_covariates.tgz\",\n",
        "          \"atlas.npz\"]\n",
        "urls = [\"https://osf.io/bqp7m/download\",\n",
        "        \"https://osf.io/s4h8j/download\",\n",
        "        \"https://osf.io/x5p4g/download\",\n",
        "        \"https://osf.io/j5kuc/download\"]\n",
        "\n",
        "for fname, url in zip(fnames, urls):\n",
        "    if not os.path.isfile(fname):\n",
        "        try:\n",
        "            r = requests.get(url)\n",
        "        except requests.ConnectionError:\n",
        "            print(\"!!! Failed to download data !!!\")\n",
        "        else:\n",
        "            if r.status_code != requests.codes.ok:\n",
        "                print(\"!!! Failed to download data !!!\")\n",
        "            else:\n",
        "                print(f\"Downloading {fname}...\")\n",
        "                with open(fname, \"wb\") as fid:\n",
        "                    fid.write(r.content)\n",
        "                print(f\"Download {fname} completed!\")\n",
        "\n",
        "\n",
        "fnames = [\"hcp_covariates\", \"hcp_rest\", \"hcp_task\"]\n",
        "\n",
        "for fname in fnames:\n",
        "    # open file\n",
        "    path_name = os.path.join(HCP_DIR, fname)\n",
        "    if not os.path.exists(path_name):\n",
        "        print(f\"Extracting {fname}.tgz...\")\n",
        "        with tarfile.open(f\"{fname}.tgz\") as fzip:\n",
        "            fzip.extractall(HCP_DIR)\n",
        "    else:\n",
        "        print(f\"File {fname}.tgz has already been extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r ./DATA"
      ],
      "metadata": {
        "id": "008kwpEKH-9U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "MOKOoL5q_lyf"
      },
      "source": [
        "## Loading region information\n",
        "\n",
        "Downloading either dataset will create the `regions.npy` file, which contains the region name and network assignment for each parcel.\n",
        "\n",
        "Detailed information about the name used for each region is provided [in the Supplement](https://static-content.springer.com/esm/art%3A10.1038%2Fnature18933/MediaObjects/41586_2016_BFnature18933_MOESM330_ESM.pdf) to [Glasser et al. 2016](https://www.nature.com/articles/nature18933).\n",
        "\n",
        "Information about the network parcellation is provided in [Ji et al, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6289683/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {},
        "id": "unoFFVlO_lyg"
      },
      "outputs": [],
      "source": [
        "dir = os.path.join(HCP_DIR, \"hcp_task\")  # choose the data directory\n",
        "regions = np.load(os.path.join(dir, \"regions.npy\")).T\n",
        "region_info = dict(name=regions[0].tolist(),\n",
        "                   network=regions[1],\n",
        "                   myelin=regions[2].astype(float)\n",
        "                   )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {},
        "id": "JDCtzxy8_lyg"
      },
      "outputs": [],
      "source": [
        "def get_image_ids(name):\n",
        "  \"\"\"Get the 1-based image indices for runs in a given experiment.\n",
        "\n",
        "    Args:\n",
        "      name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    Returns:\n",
        "      run_ids (list of int) : Numeric ID for experiment image files\n",
        "\n",
        "  \"\"\"\n",
        "  run_ids = [\n",
        "             i for i, code in enumerate(BOLD_NAMES, 1) if name.upper() in code\n",
        "             ]\n",
        "  if not run_ids:\n",
        "    raise ValueError(f\"Found no data for '{name}'\")\n",
        "  return run_ids\n",
        "\n",
        "\n",
        "def load_timeseries(subject, name, dir,\n",
        "                    runs=None, concat=True, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    name (str) : Name of experiment (\"rest\" or name of task) to load\n",
        "    dir (str) : data directory\n",
        "    run (None or int or list of ints): 0-based run(s) of the task to load,\n",
        "      or None to load all runs.\n",
        "    concat (bool) : If True, concatenate multiple runs in time\n",
        "    remove_mean (bool) : If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_tp array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  # Get the list relative 0-based index of runs to use\n",
        "  if runs is None:\n",
        "    runs = range(N_RUNS_REST) if name == \"rest\" else range(N_RUNS_TASK)\n",
        "  elif isinstance(runs, int):\n",
        "    runs = [runs]\n",
        "  # Get the first (1-based) run id for this experiment\n",
        "  offset = get_image_ids(name)[0]\n",
        "\n",
        "  # Load each run's data\n",
        "  bold_data = [\n",
        "               load_single_timeseries(subject,\n",
        "                                      offset + run,\n",
        "                                      dir,\n",
        "                                      remove_mean) for run in runs\n",
        "               ]\n",
        "\n",
        "  # Optionally concatenate in time\n",
        "  if concat:\n",
        "    bold_data = np.concatenate(bold_data, axis=-1)\n",
        "\n",
        "  return bold_data\n",
        "\n",
        "\n",
        "def load_single_timeseries(subject, bold_run, dir, remove_mean=True):\n",
        "  \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "  Args:\n",
        "    subject (int): 0-based subject ID to load\n",
        "    bold_run (int): 1-based run index, across all tasks\n",
        "    dir (str) : data directory\n",
        "    remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "\n",
        "  Returns\n",
        "    ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "  \"\"\"\n",
        "  bold_path = os.path.join(dir, \"subjects\", str(subject), \"timeseries\")\n",
        "  bold_file = f\"bold{bold_run}_Atlas_MSMAll_Glasser360Cortical.npy\"\n",
        "  ts = np.load(os.path.join(bold_path, bold_file))\n",
        "  if remove_mean:\n",
        "    ts -= ts.mean(axis=1, keepdims=True)\n",
        "  return ts\n",
        "\n",
        "\n",
        "def load_evs(subject, name, condition, dir):\n",
        "    \"\"\"Load EV (explanatory variable) data for one task condition.\n",
        "\n",
        "    Args:\n",
        "        subject (int): 0-based subject ID to load\n",
        "        name (str) : Name of task\n",
        "        condition (str) : Name of condition\n",
        "        dir (str) : data directory\n",
        "\n",
        "    Returns\n",
        "        evs (list of dicts): A dictionary with the onset, duration, and amplitude\n",
        "            of the condition for each run.\n",
        "\n",
        "    \"\"\"\n",
        "    evs = []\n",
        "    for id in get_image_ids(name):\n",
        "        task_key = BOLD_NAMES[id - 1]\n",
        "        ev_file = os.path.join(dir, \"subjects\", str(subject), \"EVs\",\n",
        "                               task_key, f\"{condition}.txt\")\n",
        "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "        ev = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "        evs.append(ev)\n",
        "    return evs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {},
        "id": "trkqq_CM_lyg"
      },
      "outputs": [],
      "source": [
        "def condition_frames(run_evs, skip=0):\n",
        "  \"\"\"Identify timepoints corresponding to a given condition in each run.\n",
        "\n",
        "  Args:\n",
        "    run_evs (list of dicts) : Onset and duration of the event, per run\n",
        "    skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "      for hemodynamic lag\n",
        "\n",
        "  Returns:\n",
        "    frames_list (list of 1D arrays): Flat arrays of frame indices, per run\n",
        "\n",
        "  \"\"\"\n",
        "  frames_list = []\n",
        "  for ev in run_evs:\n",
        "\n",
        "    # Determine when trial starts, rounded down\n",
        "    start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "\n",
        "    # Use trial duration to determine how many frames to include for trial\n",
        "    duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "\n",
        "    # Take the range of frames that correspond to this specific trial\n",
        "    frames = [s + np.arange(skip, d) for s, d in zip(start, duration)]\n",
        "\n",
        "    frames_list.append(np.concatenate(frames))\n",
        "\n",
        "  return frames_list\n",
        "\n",
        "\n",
        "def selective_average(timeseries_data, ev, skip=0, actually_average=True):\n",
        "    \"\"\"Take the temporal mean across frames for a given condition.\n",
        "\n",
        "    Args:\n",
        "        timeseries_data (array or list of arrays): n_parcel x n_tp arrays\n",
        "        ev (dict or list of dicts): Condition timing information\n",
        "        skip (int) : Ignore this many frames at the start of each trial, to account\n",
        "        for hemodynamic lag\n",
        "\n",
        "    Returns:\n",
        "        avg_data (1D array): Data averagted across selected image frames based\n",
        "        on condition timing\n",
        "\n",
        "    \"\"\"\n",
        "    # Ensure that we have lists of the same length\n",
        "    if not isinstance(timeseries_data, list):\n",
        "        timeseries_data = [timeseries_data]\n",
        "    if not isinstance(ev, list):\n",
        "        ev = [ev]\n",
        "    if len(timeseries_data) != len(ev):\n",
        "        raise ValueError(\"Length of `timeseries_data` and `ev` must match.\")\n",
        "\n",
        "    # Identify the indices of relevant frames\n",
        "    frames = condition_frames(ev, skip)\n",
        "\n",
        "    # Select the frames from each image\n",
        "    selected_data = []\n",
        "    for run_data, run_frames in zip(timeseries_data, frames):\n",
        "        run_frames = run_frames[run_frames < run_data.shape[1]]\n",
        "        selected_data.append(run_data[:, run_frames])\n",
        "\n",
        "    # Take the average in each parcel\n",
        "    if actually_average:\n",
        "        avg_data = np.concatenate(selected_data, axis=-1).mean(axis=-1)\n",
        "    else:\n",
        "        avg_data = np.concatenate(selected_data, axis=-1)\n",
        "\n",
        "    return avg_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GAMBLING"
      ],
      "metadata": {
        "id": "jp7DJ5AD2zCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_gambling = []\n",
        "for subject in subjects:\n",
        "    timeseries_gambling.append(load_timeseries(subject, \"GAMBLING\",\n",
        "                                               dir=os.path.join(HCP_DIR, \"hcp_task\"),\n",
        "                                               concat=False))"
      ],
      "metadata": {
        "id": "bJQYzAOHMTd4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"GAMBLING\"\n",
        "conditions = [\"win\", \"loss\"]\n",
        "\n",
        "win_loss = []\n",
        "for subject in subjects:\n",
        "\n",
        "    # Get the average signal in each region for each condition\n",
        "    evs = [load_evs(subject, task, cond, dir=os.path.join(HCP_DIR, \"hcp_task\")) for cond in conditions]\n",
        "    avgs = [selective_average(timeseries_gambling[subject], ev) for ev in evs]\n",
        "\n",
        "    win_loss.append(avgs)\n",
        "\n",
        "win_loss_ml = np.array(win_loss)\n",
        "win_loss_ml.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J8b7K2-Qb1O",
        "outputId": "12933b87-3811-435c-c37d-490562be8e2c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 2, 360)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Осталось только выяснить, кто из conditions win, а кто loss"
      ],
      "metadata": {
        "id": "aWMPYRnrGB8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_data = win_loss_ml[:,0,:]\n",
        "loss_data = win_loss_ml[:,1,:]\n",
        "win_data.shape, loss_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eee_MyK4GITy",
        "outputId": "7be45988-f876-4550-ae00-76f5ca419521"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((339, 360), (339, 360))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical approach"
      ],
      "metadata": {
        "id": "KJSDg6XEV02c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_data_a1e2 = np.concatenate((win_data, np.array([[1]*win_data.shape[0]]).T), axis=1)\n",
        "loss_data_a1e2 = np.concatenate((loss_data, np.array([[0]*loss_data.shape[0]]).T), axis=1)\n",
        "\n",
        "win_loss_data = np.concatenate((win_data_a1e2, loss_data_a1e2), axis=0)"
      ],
      "metadata": {
        "id": "l2wn2GQZNoc1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_gambling = win_loss_data[:,:360]\n",
        "y_gambling = win_loss_data[:,360]\n",
        "\n",
        "X_train_g, X_test_g, y_train_g, y_test_g = train_test_split(X_gambling, y_gambling, test_size=0.2, shuffle=True, random_state=88)\n",
        "\n",
        "X_gambling = np.concatenate((X_train_g, X_test_g), axis=0)\n",
        "y_gambling = np.concatenate((y_train_g, y_test_g), axis=0)"
      ],
      "metadata": {
        "id": "B7xvVkNSMBaL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go ML -> -> ->\n",
        "\n",
        "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
        "\n",
        "def nested_cv(estimator, param_grid, X, y):\n",
        "\n",
        "    model = GridSearchCV(\n",
        "        estimator, param_grid, cv=inner_cv, n_jobs=2\n",
        "    )\n",
        "\n",
        "    # Outer cross-validation to compute the testing score\n",
        "    test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=2, scoring='accuracy')\n",
        "    print(\n",
        "        \"The mean score using nested cross-validation is: \"\n",
        "        f\"{test_score.mean():.3f} ± {test_score.std():.3f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "DCZ9un2sefS7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rfc = {\n",
        "    'n_estimators': [100, 150, 200],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "gs = nested_cv(RandomForestClassifier(random_state=88), param_grid_rfc, X_gambling, y_gambling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxRkqzo6GGZ5",
        "outputId": "23af64be-2fe2-40ae-9324-8c213cc8c967"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.814 ± 0.031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rfc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "gs = nested_cv(RandomForestClassifier(random_state=88), param_grid_rfc, X_gambling, y_gambling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4jLjxQmdk5I",
        "outputId": "47fac230-658a-4d3e-e803-e5eb314dd100"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.822 ± 0.012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_abc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "}\n",
        "\n",
        "gs = nested_cv(AdaBoostClassifier(random_state=88), param_grid_abc, X_gambling, y_gambling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RU7phH2GNY1",
        "outputId": "86cc31b8-5e08-4ac0-f1ab-36f67307f9f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.832 ± 0.013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_abc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "}\n",
        "\n",
        "gs = nested_cv(AdaBoostClassifier(random_state=88), param_grid_abc, X_gambling, y_gambling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ6BEgzZfx1r",
        "outputId": "76bfd343-de9f-4464-92a8-6c7672430825"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.860 ± 0.010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lr = {\n",
        "    'C': [0.05, 0.1, 0.5, 1],\n",
        "    'penalty': ['l1','l2'],\n",
        "}\n",
        "\n",
        "ss = StandardScaler().fit(X_gambling)\n",
        "\n",
        "X_gambling_t = ss.transform(X_gambling)\n",
        "\n",
        "gs = nested_cv(LogisticRegression(random_state=88), param_grid_lr, X_gambling_t, y_gambling)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz3BTGPhgBxC",
        "outputId": "3aa7ca6d-e828-4003-8c09-9c56ab0ea256"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.914 ± 0.006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timeseries and PCA (and DL start)"
      ],
      "metadata": {
        "id": "KsiJwYtwOJhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"GAMBLING\"\n",
        "conditions = [\"win\", \"loss\"]\n",
        "\n",
        "win_loss = []\n",
        "for subject in subjects:\n",
        "\n",
        "    # Get the average signal in each region for each condition\n",
        "    evs = [load_evs(subject, task, cond, dir=os.path.join(HCP_DIR, \"hcp_task\")) for cond in conditions]\n",
        "    avgs = [selective_average(timeseries_gambling[subject], ev, actually_average=False) for ev in evs]\n",
        "\n",
        "    win_loss.append(avgs)\n",
        "\n",
        "win_loss_pca = np.array(win_loss)\n",
        "\n",
        "win_pca_a1 = win_loss_pca[:,0,:,:]\n",
        "loss_pca_a1 = win_loss_pca[:,1,:,:]\n",
        "\n",
        "win_loss_pca.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiU0WX0JcogS",
        "outputId": "bcf94567-c938-44d6-aead-886b2b21043f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 2, 360, 156)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "win_data_a2e1 = win_pca_a1.reshape(339, -1)\n",
        "loss_data_a2e1 = loss_pca_a1.reshape(339, -1)\n",
        "\n",
        "win_loss_data_a2 = np.concatenate((win_data_a2e1, loss_data_a2e1), axis=0)\n",
        "win_loss_data_a2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKNPPphJO5u5",
        "outputId": "d97765bc-0fd0-44db-8a47-3ae9418f1267"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(678, 56160)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_ts = win_loss_data_a2\n",
        "y_ts = np.array([1]*(win_loss_data_a2.shape[0]//2) + [0]*(win_loss_data_a2.shape[0]//2))"
      ],
      "metadata": {
        "id": "y4Nt0Zy0O6En"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go dim reduce -> -> ->\n",
        "\n",
        "pca = PCA(n_components=300).fit(X_ts)\n",
        "X_ts_t = pca.transform(X_ts)"
      ],
      "metadata": {
        "id": "Y5ndJn6oO6HJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rfc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "gs = nested_cv(RandomForestClassifier(random_state=88), param_grid_rfc, X_ts_t, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_-5_E91iRS2",
        "outputId": "ad5a0fdc-9502-43b4-d3ca-2883d3009579"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.850 ± 0.034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_abc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "}\n",
        "\n",
        "gs = nested_cv(AdaBoostClassifier(random_state=88), param_grid_abc, X_ts_t, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwXZnYfojFMI",
        "outputId": "6228e4fd-59d9-439f-c0ba-6af5f5e2179d"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.891 ± 0.006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lr = {\n",
        "    'C': [0.05, 0.1, 0.5, 1],\n",
        "    'penalty': ['l1','l2'],\n",
        "}\n",
        "\n",
        "ss = StandardScaler().fit(X_ts_t)\n",
        "\n",
        "X_ts_t_s = ss.transform(X_ts_t)\n",
        "\n",
        "gs = nested_cv(LogisticRegression(random_state=88), param_grid_lr, X_ts_t_s, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0lLU-R1jakb",
        "outputId": "4f4a5418-1c3b-473f-be9d-9e3e742fe91d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.941 ± 0.006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL"
      ],
      "metadata": {
        "id": "biUGobwjoUmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "lYQmJMhfxRkD"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go convolute -> -> ->\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(4, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(61504, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x.float())\n",
        "        return x"
      ],
      "metadata": {
        "id": "USkeFQQkxUig"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                         torchvision.transforms.Resize((128,128))])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.transform(self.images[idx])\n",
        "        return image.to(float), label"
      ],
      "metadata": {
        "id": "WnZSea1NxUn4"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_pca_a1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1IT0wCYklWb",
        "outputId": "f451cfac-0ede-4de5-92d0-c81f1b14f3fd"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 360, 156)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "win_loss_data_a3 = np.concatenate((win_pca_a1, loss_pca_a1), axis=0)\n",
        "labels_a3 = np.array([1]*(win_loss_data_a3.shape[0]//2) + [0]*(win_loss_data_a3.shape[0]//2))"
      ],
      "metadata": {
        "id": "HClxgbFs3vvq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "                                                                        win_loss_data_a3,\n",
        "                                                                        labels_a3,\n",
        "                                                                        test_size=0.2,\n",
        "                                                                        random_state=88,\n",
        "                                                                        shuffle=True\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "wxCmDJ9t32jN"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataset = MyDataset(images=train_images, labels=train_labels)\n",
        "test_dataset = MyDataset(images=test_images, labels=test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_dataloader,\n",
        "    'test': test_dataloader\n",
        "}"
      ],
      "metadata": {
        "id": "1lnOOCcNxUqd"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, batch_size, dataloaders, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / (len(dataloaders[phase])*batch_size)\n",
        "            epoch_acc = (running_corrects) / (len(dataloaders[phase])*batch_size)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
        "                                                        epoch_loss,\n",
        "                                                        epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vf7gvRS34jwc"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model = Net()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model_trained = train_model(model, criterion, optimizer, batch_size, dataloaders, num_epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQdEW-8C4jy6",
        "outputId": "05d34201-df3b-43c5-ef40-fb46216d67c1"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:01<00:00, 44.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 1.0177, acc: 0.7831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 125.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.3259, acc: 0.9191\n",
            "Epoch 2/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:02<00:00, 30.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.1079, acc: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 69.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.4536, acc: 0.9265\n",
            "Epoch 3/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:02<00:00, 33.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.0114, acc: 0.9963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 122.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.3979, acc: 0.9485\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Augmentation"
      ],
      "metadata": {
        "id": "zywU8RS7VFcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "win_loss_data_a4 = np.concatenate((win_pca_a1, loss_pca_a1), axis=0)\n",
        "labels_a4 = np.array([1]*(win_loss_data_a4.shape[0]//2) + [0]*(win_loss_data_a4.shape[0]//2))"
      ],
      "metadata": {
        "id": "VEiYDR57VHY-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "win_loss_data_a4.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnOWGmHolRyj",
        "outputId": "29d56298-1bff-4a33-d12a-e38963feee8d"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(678, 360, 156)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_aug(img):\n",
        "\n",
        "    aug_images = []\n",
        "    aug_labels = []\n",
        "\n",
        "    start_shape = 120\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            sample = img[:,start_shape-120:start_shape]\n",
        "            target = img[:,start_shape]\n",
        "\n",
        "            aug_images.append(sample)\n",
        "            aug_labels.append(target)\n",
        "\n",
        "            start_shape += 4\n",
        "\n",
        "        except:\n",
        "            return aug_images, aug_labels"
      ],
      "metadata": {
        "id": "xWeKpqE6axWO"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aug_images = []\n",
        "aug_labels = []\n",
        "true_labels = []\n",
        "\n",
        "for i in tqdm(range(win_loss_data_a4.shape[0])):\n",
        "\n",
        "    tl = 1 if i < win_loss_data_a4.shape[0]//2 else 0\n",
        "    ai, al = create_aug(win_loss_data_a4[i])\n",
        "\n",
        "    for _ai in ai:\n",
        "        aug_images.append(_ai)\n",
        "    for _al in al:\n",
        "        aug_labels.append(_al)\n",
        "\n",
        "    true_labels.append(tl)\n",
        "\n",
        "aug_images = np.array(aug_images)\n",
        "aug_labels = np.array(aug_labels)\n",
        "true_labels = np.array(true_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46hjYIhQdWac",
        "outputId": "b9a73e8c-bcb4-46dd-eb87-bb28de83d6b5"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 678/678 [00:00<00:00, 15921.94it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AugmentNet, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(4, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(53824, 360)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x.float())\n",
        "        return x"
      ],
      "metadata": {
        "id": "sFws3D6VVHds"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images_aug, test_images_aug, train_labels_aug, test_labels_aug = train_test_split(aug_images,\n",
        "                                                                                        aug_labels,\n",
        "                                                                                        test_size=0.2,\n",
        "                                                                                        random_state=88,\n",
        "                                                                                        shuffle=True\n",
        "                                                                                        )"
      ],
      "metadata": {
        "id": "am7B9d7pgWu7"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyAugmentationDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                         torchvision.transforms.Resize((120,120))])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.transform(self.images[idx])\n",
        "        return image.to(float), label"
      ],
      "metadata": {
        "id": "XqD7AwLXhFZj"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset_aug = MyAugmentationDataset(images=train_images_aug, labels=train_labels_aug)\n",
        "test_dataset_aug = MyAugmentationDataset(images=test_images_aug, labels=test_labels_aug)\n",
        "\n",
        "train_dataloader_aug = DataLoader(train_dataset_aug, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader_aug = DataLoader(test_dataset_aug, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloaders_aug = {\n",
        "    'train': train_dataloader_aug,\n",
        "    'test': test_dataloader_aug\n",
        "}"
      ],
      "metadata": {
        "id": "al6s6_wMg0-V"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_aug(model, criterion, optimizer, batch_size, dataloaders, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = torch.sqrt(criterion(outputs.float(), labels.float()))\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                # _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                # print(preds.shape, labels.shape)\n",
        "                running_corrects += torch.sum(outputs == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / (len(dataloaders[phase])*batch_size)\n",
        "            # epoch_acc = (running_corrects) / (len(dataloaders[phase])*batch_size)\n",
        "\n",
        "            print('{} loss: {:.4f}'.format(phase, epoch_loss))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "QeTMgTtKiqsx"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R27TX2dQkfO0",
        "outputId": "2592949f-ef88-4a72-b186-b19054507db4"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "model = AugmentNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "model_trained = train_model_aug(model, criterion, optimizer, batch_size, dataloaders_aug, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfQI5QgSgWxN",
        "outputId": "a1f35fbd-3e31-4577-8a8f-de54e91787d1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:05<00:00, 60.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 66.9973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 176.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 63.4151\n",
            "Epoch 2/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 72.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 55.2912\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 176.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 64.1351\n",
            "Epoch 3/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 65.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 39.3237\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 180.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.0379\n",
            "Epoch 4/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 72.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 30.0122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 187.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.9388\n",
            "Epoch 5/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 70.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 24.2730\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 138.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.4422\n",
            "Epoch 6/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 66.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 20.1214\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 177.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.1777\n",
            "Epoch 7/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 71.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 17.4785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 179.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.5517\n",
            "Epoch 8/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 67.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 15.8644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 129.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.2300\n",
            "Epoch 9/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 71.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 14.7211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 172.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.6228\n",
            "Epoch 10/10\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 306/306 [00:04<00:00, 72.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 13.4347\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77/77 [00:00<00:00, 185.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 65.7094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBys79eygWzk",
        "outputId": "e088a29b-86d5-4405-f024-8f1a0c09b1f2"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "189"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cTvM635OgW1y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LANGUAGE"
      ],
      "metadata": {
        "id": "6-3PLWcXa7TY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timeseries_language = []\n",
        "for subject in subjects:\n",
        "    timeseries_language.append(load_timeseries(subject, \"LANGUAGE\",\n",
        "                                               dir=os.path.join(HCP_DIR, \"hcp_task\"),\n",
        "                                               concat=False))"
      ],
      "metadata": {
        "id": "huAXsNXba9Jk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"LANGUAGE\"\n",
        "conditions = [\"math\", \"story\"]\n",
        "\n",
        "math_story = []\n",
        "for subject in subjects:\n",
        "\n",
        "    # Get the average signal in each region for each condition\n",
        "    evs = [load_evs(subject, task, cond, dir=os.path.join(HCP_DIR, \"hcp_task\")) for cond in conditions]\n",
        "    avgs = [selective_average(timeseries_language[subject], ev) for ev in evs]\n",
        "\n",
        "    math_story.append(avgs)\n",
        "\n",
        "math_story_ml = np.array(math_story)\n",
        "math_story_ml.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD7ff5gKHqcl",
        "outputId": "a1057e1e-973b-4174-9e89-d93a37318492"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 2, 360)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Осталось только выяснить, кто из conditions math, а кто story"
      ],
      "metadata": {
        "id": "Er_bcFZRnatD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "math_data = math_story_ml[:,0,:]\n",
        "story_data = math_story_ml[:,1,:]\n",
        "math_data.shape, story_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJR6dQ8LHqfO",
        "outputId": "cfce2cc8-1388-49dc-c0f3-0a0a4033b71a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((339, 360), (339, 360))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classical approach"
      ],
      "metadata": {
        "id": "BTVzuOCynhw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "math_data_a1e2 = np.concatenate((math_data, np.array([[1]*math_data.shape[0]]).T), axis=1)\n",
        "story_data_a1e2 = np.concatenate((story_data, np.array([[0]*story_data.shape[0]]).T), axis=1)\n",
        "\n",
        "math_story_data = np.concatenate((math_data_a1e2, story_data_a1e2), axis=0)"
      ],
      "metadata": {
        "id": "wusbTPrKnj9w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_language = math_story_data[:,:360]\n",
        "y_language = math_story_data[:,360]\n",
        "\n",
        "X_train_l, X_test_l, y_train_l, y_test_l = train_test_split(X_language, y_language, test_size=0.2, shuffle=True, random_state=88)\n",
        "\n",
        "X_language = np.concatenate((X_train_l, X_test_l), axis=0)\n",
        "y_language = np.concatenate((y_train_l, y_test_l), axis=0)"
      ],
      "metadata": {
        "id": "2ezxMkw-nkAH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go ML -> -> ->\n",
        "\n",
        "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
        "outer_cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
        "\n",
        "def nested_cv(estimator, param_grid, X, y):\n",
        "\n",
        "    model = GridSearchCV(\n",
        "        estimator, param_grid, cv=inner_cv, n_jobs=2\n",
        "    )\n",
        "\n",
        "    # Outer cross-validation to compute the testing score\n",
        "    test_score = cross_val_score(model, X, y, cv=outer_cv, n_jobs=2, scoring='accuracy')\n",
        "    print(\n",
        "        \"The mean score using nested cross-validation is: \"\n",
        "        f\"{test_score.mean():.3f} ± {test_score.std():.3f}\"\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Nw5-ThZMnkCT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rfc = {\n",
        "    'n_estimators': [10, 20, 30],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "rfc = RandomForestClassifier(random_state=88)\n",
        "gs = nested_cv(rfc, param_grid_rfc, X_language, y_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Gmkkp6AnkEw",
        "outputId": "40cd7330-3640-4f65-f315-3569ae213e0b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.999 ± 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_abc = {\n",
        "    'n_estimators': [10, 20, 30],\n",
        "}\n",
        "\n",
        "gs = nested_cv(AdaBoostClassifier(random_state=88), param_grid_abc, X_language, y_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC7ZkqbenkG0",
        "outputId": "50ce0980-da4c-4b62-8000-6fce0a3e9643"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.994 ± 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lr = {\n",
        "    'C': [0.05, 0.1, 0.5, 1],\n",
        "    'penalty': ['l1','l2'],\n",
        "}\n",
        "\n",
        "ss = StandardScaler().fit(X_language)\n",
        "\n",
        "X_language_t = ss.transform(X_language)\n",
        "\n",
        "gs = nested_cv(LogisticRegression(random_state=88), param_grid_lr, X_language_t, y_language)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZLCDU32nkJH",
        "outputId": "610835ae-b4e6-4200-c5cc-edda9183d453"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 1.000 ± 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Timeseries and PCA (and DL start)"
      ],
      "metadata": {
        "id": "bfn4QU51pcvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "task = \"LANGUAGE\"\n",
        "conditions = [\"math\", \"story\"]\n",
        "\n",
        "math_story = []\n",
        "for subject in subjects:\n",
        "\n",
        "    # Get the average signal in each region for each condition\n",
        "    evs = [load_evs(subject, task, cond, dir=os.path.join(HCP_DIR, \"hcp_task\")) for cond in conditions]\n",
        "    avgs = [selective_average(timeseries_language[subject], ev, actually_average=False) for ev in evs]\n",
        "\n",
        "    math_story.append(avgs)\n",
        "\n",
        "for i in range(len(math_story)):\n",
        "    for j in range(len(math_story[i])):\n",
        "        math_story[i][j] = math_story[i][j][:,:281]\n",
        "\n",
        "math_story_pca = np.array(math_story)\n",
        "\n",
        "math_pca_a1 = math_story_pca[:,0,:,:]\n",
        "story_pca_a1 = math_story_pca[:,1,:,:]\n",
        "\n",
        "math_story_pca.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsKEfnDankNg",
        "outputId": "8874459b-9860-4d95-9fa4-a0d7f4ffa3db"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(339, 2, 360, 281)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "math_data_a2e1 = math_pca_a1.reshape(339, -1)\n",
        "story_data_a2e1 = story_pca_a1.reshape(339, -1)\n",
        "\n",
        "math_story_data_a2 = np.concatenate((math_data_a2e1, story_data_a2e1), axis=0)\n",
        "math_story_data_a2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB54MYgWstnJ",
        "outputId": "e768374e-2df7-4ac8-bc96-c7d139ed0684"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(678, 101160)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_ts = math_story_data_a2\n",
        "y_ts = np.array([1]*(math_story_data_a2.shape[0]//2) + [0]*(math_story_data_a2.shape[0]//2))"
      ],
      "metadata": {
        "id": "DeHDRUMlstpr"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=300).fit(X_ts)\n",
        "X_ts_t = pca.transform(X_ts)"
      ],
      "metadata": {
        "id": "8JZaFbuNstrt"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_rfc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "    'max_depth': [2, 3, 4]\n",
        "}\n",
        "\n",
        "gs = nested_cv(RandomForestClassifier(random_state=88), param_grid_rfc, X_ts_t, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJQn45Hqstvg",
        "outputId": "566454b6-2df0-4dad-e9c2-a8b1c563f246"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.963 ± 0.012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_abc = {\n",
        "    'n_estimators': [10, 25, 50],\n",
        "}\n",
        "\n",
        "gs = nested_cv(AdaBoostClassifier(random_state=88), param_grid_abc, X_ts_t, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diaiJUviy431",
        "outputId": "360010a7-ad77-446f-9aa7-c1ef7ebb82f4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.991 ± 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid_lr = {\n",
        "    'C': [0.05, 0.1, 0.5, 1],\n",
        "    'penalty': ['l1','l2'],\n",
        "}\n",
        "\n",
        "ss = StandardScaler().fit(X_ts_t)\n",
        "\n",
        "X_ts_t_s = ss.transform(X_ts_t)\n",
        "\n",
        "gs = nested_cv(LogisticRegression(random_state=88), param_grid_lr, X_ts_t_s, y_ts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJDO0fjy46O",
        "outputId": "f062b81a-c021-44d4-8d00-48d2d17b6ab7"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The mean score using nested cross-validation is: 0.971 ± 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DL"
      ],
      "metadata": {
        "id": "JIXGUIFxzJrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Ye3DOF9ay4_Q"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go convolute -> -> ->\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(1, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(4, 4, kernel_size=(3,3)),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(222784, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x.float())\n",
        "        return x"
      ],
      "metadata": {
        "id": "0258C-BkzfDs"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, images, labels):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
        "                                                         torchvision.transforms.Resize((240,240))])\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label = self.labels[idx]\n",
        "        image = self.transform(self.images[idx])\n",
        "        return image.to(float), label"
      ],
      "metadata": {
        "id": "Qy8x4xdYzfF-"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "math_story_data_a3 = np.concatenate((math_pca_a1, story_pca_a1), axis=0)\n",
        "labels_a3 = np.array([1]*(math_story_data_a3.shape[0]//2) + [0]*(math_story_data_a3.shape[0]//2))"
      ],
      "metadata": {
        "id": "uoABjGukzfK3"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, test_images, train_labels, test_labels = train_test_split(\n",
        "                                                                        math_story_data_a3,\n",
        "                                                                        labels_a3,\n",
        "                                                                        test_size=0.2,\n",
        "                                                                        random_state=88,\n",
        "                                                                        shuffle=True\n",
        "                                                                       )"
      ],
      "metadata": {
        "id": "Pcxgkiw4zfNP"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "\n",
        "train_dataset = MyDataset(images=train_images, labels=train_labels)\n",
        "test_dataset = MyDataset(images=test_images, labels=test_labels)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "dataloaders = {\n",
        "    'train': train_dataloader,\n",
        "    'test': test_dataloader\n",
        "}"
      ],
      "metadata": {
        "id": "S34AOYkEzfPY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, batch_size, dataloaders, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'test']:\n",
        "            if phase == 'train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / (len(dataloaders[phase])*batch_size)\n",
        "            epoch_acc = (running_corrects) / (len(dataloaders[phase])*batch_size)\n",
        "\n",
        "            print('{} loss: {:.4f}, acc: {:.4f}'.format(phase,\n",
        "                                                        epoch_loss,\n",
        "                                                        epoch_acc))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "jp9bj9bWzwAY"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "model = Net()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "model_trained = train_model(model, criterion, optimizer, batch_size, dataloaders, num_epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7deDXK7rzwC9",
        "outputId": "d5c5cc7d-9d17-4e90-a3ec-de25e8b79dae"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:11<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 5.9262, acc: 0.9154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 36.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 1.0997, acc: 0.9853\n",
            "Epoch 2/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:09<00:00,  7.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.4993, acc: 0.9871\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 28.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.7664, acc: 0.9926\n",
            "Epoch 3/3\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68/68 [00:08<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss: 0.0000, acc: 0.9963\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:00<00:00, 38.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 0.7315, acc: 0.9926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}